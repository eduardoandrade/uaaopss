{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOG-universal Attacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This jupyter notebook demonstrates the use of TOG-unversal attacks on one of the state-of-the-art object detection algorithms: You Only Look Once v3 (YOLOv3) proposed by Joseph Redmon [[link]](https://pjreddie.com/media/files/papers/YOLOv3.pdf). We will demonstrate how to train a universal adversarial perturbation that can be used to perturb any input image such that the victim object detector is deceived with a designated malicious effect (e.g., object-vanishing). Since no per-input optimization is needed, TOG-universal is efficient and suitable for launching real-time attacks.\n",
    "\n",
    "We use a MobileNetV1 backbone in this example, but our library also supports other object detection algorithms (e.g., SSD and Faster R-CNN and neural architectures (e.g., Darknet53 and VGG16). For more supported models and details of the adversarial attacks, you may refer to the other notebooks and the papers provided in the repository [[link]](https://github.com/git-disl/TOG)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the Victim Detector (YOLOv3 - MobileNetV1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You can download the pretrained weights here [[link]](https://github.com/git-disl/TOG/releases/download/pm-v1.0/YOLOv3_MobileNetV1.h5). If you need to train your own model, you can refer to the repository implementing YOLOv3 in Keras [[link]](https://github.com/Adamdad/keras-YOLOv3-mobilenet). Once you have downloaded or trained the YOLOv3 detector, you need to *modify* the path in the cell below to point to the `.h5` file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "weights = 'model_weights/YOLOv3_MobileNetV1.h5'  # TODO: Change this path to the victim model weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries and initialize the victim detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_utils.preprocessing import letterbox_image_padded\n",
    "from misc_utils.visualization import visualize_detections\n",
    "from models.yolov3 import YOLOv3_MobileNetV1\n",
    "from matplotlib import pyplot as plt\n",
    "from keras import backend as K\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import datetime\n",
    "import random\n",
    "import os\n",
    "from keras.applications import VGG16\n",
    "K.clear_session()\n",
    "\n",
    "detector = YOLOv3_MobileNetV1(weights=weights)\n",
    "#detector = VGG16(weights='imagenet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the Attack Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This demonstration uses PASCAL VOC. To train TOG-universal, we randomly pick `n_samples` training images from the dataset. You can download VOC2007 [[link]](https://pjreddie.com/media/files/VOCtrainval_06-Nov-2007.tar) and VOC2012 [[link]](https://pjreddie.com/media/files/VOCtrainval_11-May-2012.tar), unzip them, and update the paths below to point to the correct location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "VOC07_path = 'research/datasets/VOCdevkit/VOC2007'  # TODO: Change this path to your VOC2007\n",
    "VOC12_path = 'research/datasets/VOCdevkit/VOC2012'  # TODO: Change this path to your VOC2012\n",
    "\n",
    "eps = 8 / 255.        # Hyperparameter: epsilon in L-inf norm\n",
    "eps_iter = 0.0001     # Hyperparameter: attack learning rate\n",
    "n_epochs = 50         # Hyperparameter: number of attack iterations\n",
    "#n_epochs = 49         # Hyperparameter: number of attack iterations\n",
    "n_samples = 12800      # Hyperparameter: number of training samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the paths and hyperparameters, we can then load the training images for TOG-universal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpaths_train = []  # Load image paths\n",
    "for prefix in [VOC07_path, VOC12_path]:\n",
    "    with open('%s/ImageSets/Main/trainval.txt' % prefix, 'r') as f:\n",
    "        fpaths_train += [os.path.join(prefix, 'JPEGImages', '%s.jpg' % fname.strip()) for fname in f.readlines()]\n",
    "random.shuffle(fpaths_train)  # Shuffle the image paths for random sampling\n",
    "fpaths_train = fpaths_train[:n_samples]  # Select only n_samples images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train TOG-universal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this demonstration, we conduct TOG-universal attacks with an object-vanishing effect. To start training the universal adversarial perturbation, we first randomly initialize `eta` based on the maximum distortion `eps` allowed in the hyperparameter setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "eta = np.random.uniform(-eps, eps, size=(*detector.model_img_size, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(eta, epoch, filename=\"adversarial_checkpoint.npz\"):\n",
    "    np.savez(filename, eta=eta, epoch=epoch)\n",
    "\n",
    "def load_checkpoint(filename=\"adversarial_checkpoint.npz\"):\n",
    "    if os.path.isfile(filename):\n",
    "        data = np.load(filename)\n",
    "        eta = data['eta']\n",
    "        start_epoch = data['epoch']\n",
    "        print(f\"Loaded checkpoint (epoch {start_epoch})\")\n",
    "        return eta, start_epoch\n",
    "    else:\n",
    "        print(\"No checkpoint found.\")\n",
    "        return None, 0\n",
    "\n",
    "eta_initial = 0  # Initialize this based on your scenario\n",
    "eta, start_epoch = load_checkpoint()\n",
    "\n",
    "if eta is None:\n",
    "    eta = eta_initial  # or however you initially compute or set `eta`\n",
    "\n",
    "for epoch in range(start_epoch, n_epochs):\n",
    "    pbar = tqdm(fpaths_train)\n",
    "    pbar.set_description('Epoch %d/%d' % (epoch + 1, n_epochs))\n",
    "    \n",
    "    for fpath in pbar:\n",
    "        input_img = Image.open(fpath)\n",
    "        x_query, x_meta = letterbox_image_padded(input_img, size=detector.model_img_size)\n",
    "        x_adv = np.clip(x_query + eta, 0.0, 1.0)                  # Apply the current eta\n",
    "        grad = detector.compute_object_vanishing_gradient(x_adv)  # Conduct one-step SGD\n",
    "        signed_grad = np.sign(grad[0])\n",
    "        eta = np.clip(eta - eps_iter * signed_grad, -eps, eps)    # Update eta\n",
    "\n",
    "    random.shuffle(fpaths_train)\n",
    "    save_checkpoint(eta, epoch + 1)  # Save after each epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TOG-universal iteratively optimizes the universal adversarial perturbation `eta` with `n_epochs` epochs. For more descriptions about the algorithm, you can refer to our paper [[link]](https://khchow.com/media/arXiv_TOG.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    pbar = tqdm(fpaths_train)\n",
    "    pbar.set_description('Epoch %d/%d' % (epoch + 1, n_epochs))\n",
    "    \n",
    "    for fpath in pbar:\n",
    "        input_img = Image.open(fpath)\n",
    "        x_query, x_meta = letterbox_image_padded(input_img, size=detector.model_img_size)\n",
    "\n",
    "        x_adv = np.clip(x_query + eta, 0.0, 1.0)                  # Step 1: Apply the current eta\n",
    "        grad = detector.compute_object_vanishing_gradient(x_adv)  # Step 2: Conduct one-step SGD\n",
    "        signed_grad = np.sign(grad[0])\n",
    "        eta = np.clip(eta - eps_iter * signed_grad, -eps, eps)    # Step 3: Extract the new eta\n",
    "\n",
    "    random.shuffle(fpaths_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After `n_epochs` epochs of training, `eta` is now the universal adversarial perturbation that can be used to distort any input image to the victim detector. Since we use object-vanishing to be the designated malicious effect, the victim detector will be incapable of recognizing the existence of objects under TOG-universal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.title('Trained TOG-universal: eta')\n",
    "plt.imshow((eta - eta.min()) / (eta.max() - eta.min()))\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Test TOG-universal: The Same Perturbation to Distort Different Input Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first use the trained universal adversarial perturbation (i.e., `eta`) to perturb an image that is not in the training set. We can observe that while the detector can recognize the dog perfectly in the benign scenario (left), it cannot detect any object given an input image perturbed by the TOG-universal (right)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "input_img = Image.open('./assets/example_1.jpg')\n",
    "x_query, x_meta = letterbox_image_padded(input_img, size=detector.model_img_size)\n",
    "x_adv_vanishing = np.clip(x_query + eta, 0.0, 1.0)\n",
    "detections_query = detector.detect(x_query, conf_threshold=detector.confidence_thresh_default)\n",
    "detections_adv_vanishing = detector.detect(x_adv_vanishing, conf_threshold=detector.confidence_thresh_default)\n",
    "visualize_detections({'Benign (No Attack)': (x_query, detections_query, detector.model_img_size, detector.classes),\n",
    "                      'TOG-universal': (x_adv_vanishing, detections_adv_vanishing, detector.model_img_size, detector.classes)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then apply the same universal adversarial perturbation used above to another image, which is also not one of the images in PASCAL VOC. The person can no longer be detected by the victim under TOG-universal (right). This indicates that the same perturbation can be used to perturb different input images with consistent adversarial effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = Image.open('./assets/example_2.png')\n",
    "x_query, x_meta = letterbox_image_padded(input_img, size=detector.model_img_size)\n",
    "x_adv_vanishing = np.clip(x_query + eta, 0.0, 1.0)\n",
    "detections_query = detector.detect(x_query, conf_threshold=detector.confidence_thresh_default)\n",
    "detections_adv_vanishing = detector.detect(x_adv_vanishing, conf_threshold=detector.confidence_thresh_default)\n",
    "visualize_detections({'Benign (No Attack)': (x_query, detections_query, detector.model_img_size, detector.classes),\n",
    "                      'TOG-universal': (x_adv_vanishing, detections_adv_vanishing, detector.model_img_size, detector.classes)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the universal adversarial pertubation in all images in a directory and saving them as distractor images when there is no detection after TOG-universal pertubation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that the output folder exists\n",
    "output_folder = './test'\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# List all .jpg files in the input directory\n",
    "input_folder = './test_input'\n",
    "image_files = [f for f in os.listdir(input_folder) if f.endswith('.jpg')]\n",
    "\n",
    "detected_images = []  # List to store names of images with detections\n",
    "\n",
    "# Load the distractor image (make sure this is a valid path to your distractor image)\n",
    "distractor_image_path = './assets/example_2.png'\n",
    "distractor_image = Image.open(distractor_image_path)\n",
    "\n",
    "for image_file in image_files:\n",
    "    input_img_path = os.path.join(input_folder, image_file)\n",
    "    output_img_path = os.path.join(output_folder, image_file)\n",
    "\n",
    "    # Load the image\n",
    "    input_img = Image.open(input_img_path)\n",
    "    x_query, x_meta = letterbox_image_padded(input_img, size=detector.model_img_size)\n",
    "\n",
    "    # Generate the adversarial example\n",
    "    x_adv_vanishing = np.clip(x_query + eta, 0.0, 1.0)\n",
    "\n",
    "    # Perform detection on both original and adversarial images\n",
    "    detections_query = detector.detect(x_query, conf_threshold=detector.confidence_thresh_default)\n",
    "    detections_adv_vanishing = detector.detect(x_adv_vanishing, conf_threshold=detector.confidence_thresh_default)\n",
    "\n",
    "    # Check if there are any detections; if not, create and save a resized distractor image\n",
    "    if len(detections_adv_vanishing) == 0:\n",
    "        # Resize the distractor image to match the size of the input image\n",
    "        resized_distractor = distractor_image.resize(input_img.size, Image.ANTIALIAS)\n",
    "        # Save the resized distractor image as a JPG\n",
    "        resized_distractor.save(output_img_path, format='JPEG')\n",
    "    else:\n",
    "        # Save the original image if detections were found\n",
    "        detected_images.append(image_file)\n",
    "        input_img.save(output_img_path)\n",
    "        \n",
    "# Write the list of detected images to a file\n",
    "with open('detected_query_images_cuhk_sysu_tog.txt', 'w') as file:\n",
    "    for image_name in detected_images:\n",
    "        file.write(image_name + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the universal adversarial pertubation in all images in a directory and just saving the generated images after it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that the output folder exists\n",
    "output_folder = './query_prw_p_fgsm_tog_iou'\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# List all .jpg files in the input directory\n",
    "input_folder = './query_prw_p_fgsm_iou'\n",
    "image_files = [f for f in os.listdir(input_folder) if f.endswith('.jpg')]\n",
    "\n",
    "for image_file in image_files:\n",
    "    input_img_path = os.path.join(input_folder, image_file)\n",
    "    output_img_path = os.path.join(output_folder, image_file)\n",
    "\n",
    "    # Load the image\n",
    "    input_img = Image.open(input_img_path)\n",
    "    original_size = input_img.size\n",
    "\n",
    "    # Convert the image to an array for processing\n",
    "    input_array = np.array(input_img) / 255.0  # Normalize to 0-1 range\n",
    "\n",
    "    # Resize eta to match the current image dimensions\n",
    "    eta_resized = np.resize(eta, (original_size[1], original_size[0], 3))  # Note the reversal in dimensions for numpy\n",
    "\n",
    "    # Generate the adversarial example\n",
    "    x_adv_vanishing = np.clip(input_array + eta_resized, 0.0, 1.0)  # Apply adversarial perturbation\n",
    "\n",
    "    # Convert adversarial example back to an image\n",
    "    adv_img = Image.fromarray((x_adv_vanishing * 255).astype(np.uint8))\n",
    "    \n",
    "    # Assume visualize_detections now returns a PIL Image object\n",
    "    #output_img = visualize_detections({'Benign (No Attack)': (x_query, detections_query, detector.model_img_size, detector.classes),\n",
    "                                           #'TOG-universal': (x_adv_vanishing, detections_adv_vanishing, detector.model_img_size, detector.classes)})\n",
    "\n",
    "    # Save the adversarial image\n",
    "    adv_img.save(output_img_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
